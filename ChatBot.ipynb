{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import time\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "lines = open('movie_lines.txt').read().split('\\n')\n",
    "conv_lines = open('movie_conversations.txt').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match line id with text\n",
    "id2line = {}\n",
    "for line in lines:\n",
    "    _line = line.split(' +++$+++ ')\n",
    "    if len(_line) == 5: #keeping only sentences with >5 words\n",
    "        id2line[_line[0]] = _line[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all of the conversations' lines' ids.\n",
    "convs = [ ]\n",
    "for line in conv_lines[:-1]:\n",
    "    _line = line.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n",
    "    convs.append(_line.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "convs[:10] #previewing conversations\n",
    "convs=convs[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order the sentences into questions and answers\n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "for conv in convs:\n",
    "    for i in range(len(conv)-1):\n",
    "        questions.append(id2line[conv[i]])\n",
    "        answers.append(id2line[conv[i+1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\n",
      "Well, I thought we'd start with pronunciation, if that's okay with you.\n",
      " \n",
      "Well, I thought we'd start with pronunciation, if that's okay with you.\n",
      "Not the hacking and gagging and spitting part.  Please.\n",
      " \n",
      "Not the hacking and gagging and spitting part.  Please.\n",
      "Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\n",
      " \n",
      "You're asking me out.  That's so cute. What's your name again?\n",
      "Forget it.\n",
      " \n",
      "No, no, it's my fault -- we didn't have a proper introduction ---\n",
      "Cameron.\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Examples\n",
    "limit = 0\n",
    "for i in range(limit, limit+5):\n",
    "    print(questions[i])\n",
    "    print(answers[i])\n",
    "    print \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing shortened words, punctuation\n",
    "def clean_text(text):\n",
    "\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to questions and answers\n",
    "clean_questions = []\n",
    "for question in questions:\n",
    "    clean_questions.append(clean_text(question))\n",
    "    \n",
    "clean_answers = []    \n",
    "for answer in answers:\n",
    "    clean_answers.append(clean_text(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can we make this quick  roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad  again\n",
      "well i thought we would start with pronunciation if that is okay with you\n",
      " \n",
      "well i thought we would start with pronunciation if that is okay with you\n",
      "not the hacking and gagging and spitting part  please\n",
      " \n",
      "not the hacking and gagging and spitting part  please\n",
      "okay then how about we try out some french cuisine  saturday  night\n",
      " \n",
      "you are asking me out  that is so cute that is your name again\n",
      "forget it\n",
      " \n",
      "no no it is my fault  we did not have a proper introduction \n",
      "cameron\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Preview cleaned data\n",
    "limit = 0\n",
    "for i in range(limit, limit+5):\n",
    "    print(clean_questions[i])\n",
    "    print(clean_answers[i])\n",
    "    print \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspecting lengths of sentences\n",
    "lengths = []\n",
    "for question in clean_questions:\n",
    "    lengths.append(len(question.split()))\n",
    "for answer in clean_answers:\n",
    "    lengths.append(len(answer.split()))\n",
    "\n",
    "lengths = pd.DataFrame(lengths, columns=['counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50298.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10.883574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>11.909324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>498.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             counts\n",
       "count  50298.000000\n",
       "mean      10.883574\n",
       "std       11.909324\n",
       "min        0.000000\n",
       "25%        4.000000\n",
       "50%        7.000000\n",
       "75%       14.000000\n",
       "max      498.000000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.0\n",
      "19.0\n",
      "24.0\n",
      "32.0\n",
      "57.0\n"
     ]
    }
   ],
   "source": [
    "print(np.percentile(lengths, 80))\n",
    "print(np.percentile(lengths, 85))\n",
    "print(np.percentile(lengths, 90))\n",
    "print(np.percentile(lengths, 95))\n",
    "print(np.percentile(lengths, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove questions and answers that are shorter than 2 words and longer than 20 words.\n",
    "min_line_length = 2\n",
    "max_line_length = 20\n",
    "\n",
    "short_questions_temp = []\n",
    "short_answers_temp = []\n",
    "\n",
    "i = 0\n",
    "for question in clean_questions:\n",
    "    if len(question.split()) >= min_line_length and len(question.split()) <= max_line_length:\n",
    "        short_questions_temp.append(question)\n",
    "        short_answers_temp.append(clean_answers[i])\n",
    "    i += 1\n",
    "\n",
    "\n",
    "short_questions = []\n",
    "short_answers = []\n",
    "\n",
    "i = 0\n",
    "for answer in short_answers_temp:\n",
    "    if len(answer.split()) >= min_line_length and len(answer.split()) <= max_line_length:\n",
    "        short_answers.append(answer)\n",
    "        short_questions.append(short_questions_temp[i])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary\n",
    "#Contains word--->word frequency\n",
    "vocab = {}\n",
    "for question in short_questions:\n",
    "    for word in question.split():\n",
    "        if word not in vocab:\n",
    "            vocab[word] = 1\n",
    "        else:\n",
    "            vocab[word] += 1\n",
    "            \n",
    "for answer in short_answers:\n",
    "    for word in answer.split():\n",
    "        if word not in vocab:\n",
    "            vocab[word] = 1\n",
    "        else:\n",
    "            vocab[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only keeping words that occure more than 10 times\n",
    "threshold = 10\n",
    "\n",
    "#Create dictionaries to provide a unique integer for each word\n",
    "#This is a needed step in order to create our embeddings later\n",
    "questions_vocab_to_int = {}\n",
    "\n",
    "word_num = 0\n",
    "for word, count in vocab.items():\n",
    "    if count >= threshold:\n",
    "        questions_vocab_to_int[word] = word_num\n",
    "        word_num += 1\n",
    "        \n",
    "answers_vocab_to_int = {}\n",
    "\n",
    "word_num = 0\n",
    "for word, count in vocab.items():\n",
    "    if count >= threshold:\n",
    "        answers_vocab_to_int[word] = word_num\n",
    "        word_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the unique tokens to the vocabulary dictionaries.\n",
    "codes = ['<PAD>','<EOS>','<UNK>','<GO>']\n",
    "\n",
    "for code in codes:\n",
    "    questions_vocab_to_int[code] = len(questions_vocab_to_int)+1\n",
    "    \n",
    "for code in codes:\n",
    "    answers_vocab_to_int[code] = len(answers_vocab_to_int)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries to map the unique integers to their respective words.\n",
    "# i.e. an inverse dictionary for vocab_to_int.\n",
    "questions_int_to_vocab = {v_i: v for v, v_i in questions_vocab_to_int.items()}\n",
    "answers_int_to_vocab = {v_i: v for v, v_i in answers_vocab_to_int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1729\n",
      "1729\n",
      "1729\n",
      "1729\n"
     ]
    }
   ],
   "source": [
    "# Check the length of the dictionaries.\n",
    "print(len(questions_vocab_to_int))\n",
    "print(len(questions_int_to_vocab))\n",
    "print(len(answers_vocab_to_int))\n",
    "print(len(answers_int_to_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the end of sentence token to the end of every answer.\n",
    "for i in range(len(short_answers)):\n",
    "    short_answers[i] += ' <EOS>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating sentences of word Ids\n",
    "#Add UNK tag to rare (removed from vocabulary) words\n",
    "\n",
    "questions_int = []\n",
    "for question in short_questions:\n",
    "    ints = []\n",
    "    for word in question.split():\n",
    "        if word not in questions_vocab_to_int:\n",
    "            ints.append(questions_vocab_to_int['<UNK>'])\n",
    "        else:\n",
    "            ints.append(questions_vocab_to_int[word])\n",
    "    questions_int.append(ints)\n",
    "    \n",
    "answers_int = []\n",
    "for answer in short_answers:\n",
    "    ints = []\n",
    "    for word in answer.split():\n",
    "        if word not in answers_vocab_to_int:\n",
    "            ints.append(answers_vocab_to_int['<UNK>'])\n",
    "        else:\n",
    "            ints.append(answers_vocab_to_int[word])\n",
    "    answers_int.append(ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15724\n",
      "15724\n"
     ]
    }
   ],
   "source": [
    "# Check the lengths\n",
    "print(len(questions_int))\n",
    "print(len(answers_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort questions and answers by the length of questions.\n",
    "\n",
    "sorted_questions = []\n",
    "sorted_answers = []\n",
    "\n",
    "for length in range(1, max_line_length+1):\n",
    "    for i in enumerate(questions_int):\n",
    "        if len(i[1]) == length:\n",
    "            sorted_questions.append(questions_int[i[0]])\n",
    "            sorted_answers.append(answers_int[i[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    #Setting up model's variables\n",
    "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "    return input_data, targets, lr, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_encoding_input(target_data, vocab_to_int, batch_size):\n",
    "    #Add GO tag to start of each sentence\n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
    "\n",
    "    return dec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob, sequence_length):\n",
    "    #Using Multilayer Bidirectional LSTM RNN cells with dropout\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
    "    enc_cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "    _, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw = enc_cell,\n",
    "                                                   cell_bw = enc_cell,\n",
    "                                                   sequence_length = sequence_length,\n",
    "                                                   inputs = rnn_inputs, \n",
    "                                                   dtype=tf.float32)\n",
    "    return enc_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer_train(encoder_state, dec_cell, dec_embed_input, sequence_length, decoding_scope,\n",
    "                         output_fn, keep_prob, batch_size):\n",
    "    #training part of decoder\n",
    "    #using attention to handle context in long sentences\n",
    "    #getting as input the encoder's output\n",
    "    \n",
    "    attention_states = tf.zeros([batch_size, 1, dec_cell.output_size])\n",
    "    \n",
    "    att_keys, att_vals, att_score_fn, att_construct_fn = \\\n",
    "            tf.contrib.seq2seq.prepare_attention(attention_states,\n",
    "                                                 attention_option=\"bahdanau\",\n",
    "                                                 num_units=dec_cell.output_size)\n",
    "    \n",
    "    train_decoder_fn = tf.contrib.seq2seq.attention_decoder_fn_train(encoder_state[0],\n",
    "                                                                     att_keys,\n",
    "                                                                     att_vals,\n",
    "                                                                     att_score_fn,\n",
    "                                                                     att_construct_fn,\n",
    "                                                                     name = \"attn_dec_train\")\n",
    "    train_pred, _, _ = tf.contrib.seq2seq.dynamic_rnn_decoder(dec_cell, \n",
    "                                                              train_decoder_fn, \n",
    "                                                              dec_embed_input, \n",
    "                                                              sequence_length, \n",
    "                                                              scope=decoding_scope)\n",
    "    train_pred_drop = tf.nn.dropout(train_pred, keep_prob)\n",
    "    return output_fn(train_pred_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id, end_of_sequence_id,\n",
    "                         maximum_length, vocab_size, decoding_scope, output_fn, keep_prob, batch_size):\n",
    "    #Similar to decoder used for training, just without adding the dropout\n",
    "    #Also, since we are predicting answers, we have no labels\n",
    "    #Same embedding parameters (only) used as for training\n",
    "    attention_states = tf.zeros([batch_size, 1, dec_cell.output_size])\n",
    "    \n",
    "    att_keys, att_vals, att_score_fn, att_construct_fn = \\\n",
    "            tf.contrib.seq2seq.prepare_attention(attention_states,\n",
    "                                                 attention_option=\"bahdanau\",\n",
    "                                                 num_units=dec_cell.output_size)\n",
    "    \n",
    "    infer_decoder_fn = tf.contrib.seq2seq.attention_decoder_fn_inference(output_fn, \n",
    "                                                                         encoder_state[0], \n",
    "                                                                         att_keys, \n",
    "                                                                         att_vals, \n",
    "                                                                         att_score_fn, \n",
    "                                                                         att_construct_fn, \n",
    "                                                                         dec_embeddings,\n",
    "                                                                         start_of_sequence_id, \n",
    "                                                                         end_of_sequence_id, \n",
    "                                                                         maximum_length, \n",
    "                                                                         vocab_size, \n",
    "                                                                         name = \"attn_dec_inf\")\n",
    "    infer_logits, _, _ = tf.contrib.seq2seq.dynamic_rnn_decoder(dec_cell, \n",
    "                                                                infer_decoder_fn, \n",
    "                                                                scope=decoding_scope)\n",
    "    \n",
    "    return infer_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer(dec_embed_input, dec_embeddings, encoder_state, vocab_size, sequence_length, rnn_size,\n",
    "                   num_layers, vocab_to_int, keep_prob, batch_size):\n",
    "    #The whole decoding layer\n",
    "    \n",
    "    with tf.variable_scope(\"decoding\") as decoding_scope:\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
    "        dec_cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "        \n",
    "        weights = tf.truncated_normal_initializer(stddev=0.1)\n",
    "        biases = tf.zeros_initializer()\n",
    "        output_fn = lambda x: tf.contrib.layers.fully_connected(x, \n",
    "                                                                vocab_size, \n",
    "                                                                None, \n",
    "                                                                scope=decoding_scope,\n",
    "                                                                weights_initializer = weights,\n",
    "                                                                biases_initializer = biases)\n",
    "\n",
    "        train_logits = decoding_layer_train(encoder_state, \n",
    "                                            dec_cell, \n",
    "                                            dec_embed_input, \n",
    "                                            sequence_length, \n",
    "                                            decoding_scope, \n",
    "                                            output_fn, \n",
    "                                            keep_prob, \n",
    "                                            batch_size)\n",
    "        decoding_scope.reuse_variables()\n",
    "        infer_logits = decoding_layer_infer(encoder_state, \n",
    "                                            dec_cell, \n",
    "                                            dec_embeddings, \n",
    "                                            vocab_to_int['<GO>'],\n",
    "                                            vocab_to_int['<EOS>'], \n",
    "                                            sequence_length - 1, \n",
    "                                            vocab_size,\n",
    "                                            decoding_scope, \n",
    "                                            output_fn, keep_prob, \n",
    "                                            batch_size)\n",
    "\n",
    "    return train_logits, infer_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, batch_size, sequence_length, answers_vocab_size, \n",
    "                  questions_vocab_size, enc_embedding_size, dec_embedding_size, rnn_size, num_layers, \n",
    "                  questions_vocab_to_int):\n",
    "    \n",
    "    #Sequence to sequence model\n",
    "    #Encoding + decoding layers connected\n",
    "    #Embeddings implemented \n",
    "    \n",
    "    enc_embed_input = tf.contrib.layers.embed_sequence(input_data, \n",
    "                                                       answers_vocab_size+1, \n",
    "                                                       enc_embedding_size,\n",
    "                                                       initializer = tf.random_uniform_initializer(0,1))\n",
    "    enc_state = encoding_layer(enc_embed_input, rnn_size, num_layers, keep_prob, sequence_length)\n",
    "\n",
    "    dec_input = process_encoding_input(target_data, questions_vocab_to_int, batch_size)\n",
    "    dec_embeddings = tf.Variable(tf.random_uniform([questions_vocab_size+1, dec_embedding_size], 0, 1))\n",
    "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
    "    \n",
    "    train_logits, infer_logits = decoding_layer(dec_embed_input, \n",
    "                                                dec_embeddings, \n",
    "                                                enc_state, \n",
    "                                                questions_vocab_size, \n",
    "                                                sequence_length, \n",
    "                                                rnn_size, \n",
    "                                                num_layers, \n",
    "                                                questions_vocab_to_int, \n",
    "                                                keep_prob, \n",
    "                                                batch_size)\n",
    "    return train_logits, infer_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Hyperparameters\n",
    "epochs = 6\n",
    "batch_size = 64\n",
    "rnn_size = 512\n",
    "num_layers = 2\n",
    "encoding_embedding_size = 512\n",
    "decoding_embedding_size = 512\n",
    "learning_rate = 0.005\n",
    "learning_rate_decay = 0.9\n",
    "min_learning_rate = 0.0001\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model inputs    \n",
    "input_data, targets, lr, keep_prob = model_inputs()\n",
    "# Sequence length is max sentence length\n",
    "sequence_length = tf.placeholder_with_default(max_line_length, None, name='sequence_length')\n",
    "input_shape = tf.shape(input_data)\n",
    "\n",
    "# Create the training and inference predictions (aka logits)\n",
    "train_logits, inference_logits = seq2seq_model(\n",
    "    tf.reverse(input_data, [-1]), targets, keep_prob, batch_size, sequence_length, len(answers_vocab_to_int), \n",
    "    len(questions_vocab_to_int), encoding_embedding_size, decoding_embedding_size, rnn_size, num_layers, \n",
    "    questions_vocab_to_int)\n",
    "\n",
    "with tf.name_scope(\"optimization\"):\n",
    "    # Weighted softmax cross entropy  \n",
    "    cost = tf.contrib.seq2seq.sequence_loss(\n",
    "        train_logits,\n",
    "        targets,\n",
    "        tf.ones([input_shape[0], sequence_length]))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "    # Gradient Clipping\n",
    "    #getting the gradient values from the optimizer\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    #normalize gradients to be in our boundaries\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "    #putting gradients back to the optimizer\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch, vocab_to_int):\n",
    "    #add PAD tag to sentences with lebgth smaller than our max length\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(questions, answers, batch_size):\n",
    "    #Generator to get the batches\n",
    "    for batch_i in range(0, len(questions)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        questions_batch = questions[start_i:start_i + batch_size]\n",
    "        answers_batch = answers[start_i:start_i + batch_size]\n",
    "        pad_questions_batch = np.array(pad_sentence_batch(questions_batch, questions_vocab_to_int))\n",
    "        pad_answers_batch = np.array(pad_sentence_batch(answers_batch, answers_vocab_to_int))\n",
    "        yield pad_questions_batch, pad_answers_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13366\n",
      "2358\n"
     ]
    }
   ],
   "source": [
    "train_valid_split = int(len(sorted_questions)*0.15)\n",
    "\n",
    "# Split the questions and answers into train and test\n",
    "train_questions = sorted_questions[train_valid_split:]\n",
    "train_answers = sorted_answers[train_valid_split:]\n",
    "\n",
    "valid_questions = sorted_questions[:train_valid_split]\n",
    "valid_answers = sorted_answers[:train_valid_split]\n",
    "\n",
    "print(len(train_questions))\n",
    "print(len(valid_questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/6 Batch    0/208 - Loss:  0.075, Seconds: 316.69\n",
      "Epoch   1/6 Batch  100/208 - Loss:  2.979, Seconds: 343.48\n",
      "Valid Loss:  2.176, Seconds: 33.98\n",
      "New Record!\n",
      "Epoch   1/6 Batch  200/208 - Loss:  2.220, Seconds: 422.78\n",
      "Valid Loss:  2.130, Seconds: 27.78\n",
      "New Record!\n",
      "Epoch   2/6 Batch    0/208 - Loss:  0.173, Seconds: 240.66\n",
      "Epoch   2/6 Batch  100/208 - Loss:  2.089, Seconds: 301.72\n",
      "Valid Loss:  2.044, Seconds: 25.55\n",
      "New Record!\n",
      "Epoch   2/6 Batch  200/208 - Loss:  2.091, Seconds: 450.88\n",
      "Valid Loss:  2.013, Seconds: 25.30\n",
      "New Record!\n",
      "Epoch   3/6 Batch    0/208 - Loss:  0.163, Seconds: 269.71\n",
      "Epoch   3/6 Batch  100/208 - Loss:  1.978, Seconds: 332.58\n",
      "Valid Loss:  1.952, Seconds: 28.75\n",
      "New Record!\n",
      "Epoch   3/6 Batch  200/208 - Loss:  2.009, Seconds: 464.90\n",
      "Valid Loss:  1.943, Seconds: 28.99\n",
      "New Record!\n",
      "Epoch   4/6 Batch    0/208 - Loss:  0.158, Seconds: 468.98\n",
      "Epoch   4/6 Batch  100/208 - Loss:  1.922, Seconds: 457.48\n",
      "Valid Loss:  1.909, Seconds: 28.90\n",
      "New Record!\n",
      "Epoch   4/6 Batch  200/208 - Loss:  1.961, Seconds: 467.78\n",
      "Valid Loss:  1.901, Seconds: 25.99\n",
      "New Record!\n",
      "Epoch   5/6 Batch    0/208 - Loss:  0.154, Seconds: 266.66\n",
      "Epoch   5/6 Batch  100/208 - Loss:  1.886, Seconds: 326.73\n",
      "Valid Loss:  1.878, Seconds: 25.23\n",
      "New Record!\n",
      "Epoch   5/6 Batch  200/208 - Loss:  1.928, Seconds: 473.32\n",
      "Valid Loss:  1.875, Seconds: 25.09\n",
      "New Record!\n",
      "Epoch   6/6 Batch    0/208 - Loss:  0.152, Seconds: 277.57\n",
      "Epoch   6/6 Batch  100/208 - Loss:  1.856, Seconds: 344.27\n",
      "Valid Loss:  1.861, Seconds: 25.15\n",
      "New Record!\n",
      "Epoch   6/6 Batch  200/208 - Loss:  1.904, Seconds: 481.32\n",
      "Valid Loss:  1.866, Seconds: 25.11\n",
      "No Improvement.\n"
     ]
    }
   ],
   "source": [
    "display_step = 100 # Check training loss after every 100 batches\n",
    "stop_early = 0 \n",
    "stop = 5 # If the validation loss does decrease in 5 consecutive checks, stop training\n",
    "validation_check = ((len(train_questions))//batch_size//2)-1 # Modulus for checking validation loss\n",
    "total_train_loss = 0 # Record the training loss for each display step\n",
    "summary_valid_loss = [] # Record the validation loss for saving improvements in the model\n",
    "\n",
    "graph = tf.Graph().as_default()\n",
    "saver = tf.train.Saver()  # create the saver after the graph\n",
    "with tf.Session() as sess:  # Session object\n",
    "                    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(1, epochs+1):\n",
    "        for batch_i, (questions_batch, answers_batch) in enumerate(\n",
    "            batch_data(train_questions, train_answers, batch_size)):\n",
    "            start_time = time.time()\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: questions_batch,\n",
    "                 targets: answers_batch,\n",
    "                 lr: learning_rate,\n",
    "                 sequence_length: answers_batch.shape[1],\n",
    "                 keep_prob: keep_probability})\n",
    "\n",
    "            total_train_loss += loss\n",
    "            end_time = time.time()\n",
    "            batch_time = end_time - start_time\n",
    "\n",
    "            if batch_i % display_step == 0:\n",
    "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                      .format(epoch_i,\n",
    "                              epochs, \n",
    "                              batch_i, \n",
    "                              len(train_questions) // batch_size, \n",
    "                              total_train_loss / display_step, \n",
    "                              batch_time*display_step))\n",
    "                total_train_loss = 0\n",
    "\n",
    "            if batch_i % validation_check == 0 and batch_i > 0:\n",
    "                total_valid_loss = 0\n",
    "                start_time = time.time()\n",
    "                for batch_ii, (questions_batch, answers_batch) in \\\n",
    "                        enumerate(batch_data(valid_questions, valid_answers, batch_size)):\n",
    "                    valid_loss = sess.run(\n",
    "                    cost, {input_data: questions_batch,\n",
    "                           targets: answers_batch,\n",
    "                           lr: learning_rate,\n",
    "                           sequence_length: answers_batch.shape[1],\n",
    "                           keep_prob: 1})\n",
    "                    total_valid_loss += valid_loss\n",
    "                end_time = time.time()\n",
    "                batch_time = end_time - start_time\n",
    "                avg_valid_loss = total_valid_loss / (len(valid_questions) / batch_size)\n",
    "                print('Valid Loss: {:>6.3f}, Seconds: {:>5.2f}'.format(avg_valid_loss, batch_time))\n",
    "\n",
    "                # Reduce learning rate within limit\n",
    "                learning_rate *= learning_rate_decay\n",
    "                if learning_rate < min_learning_rate:\n",
    "                    learning_rate = min_learning_rate\n",
    "\n",
    "                summary_valid_loss.append(avg_valid_loss)\n",
    "                if avg_valid_loss <= min(summary_valid_loss):\n",
    "                    print('New Record!') \n",
    "                    stop_early = 0\n",
    "                    saver.save(sess, './my-model') #if we see an impovement, we save the model\n",
    "\n",
    "                else:\n",
    "                    print(\"No Improvement.\")\n",
    "                    stop_early += 1\n",
    "                    if stop_early == stop:\n",
    "                        break\n",
    "\n",
    "        if stop_early == stop:\n",
    "            print(\"Stopping Training.\")\n",
    "            break\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_to_seq(question, vocab_to_int):\n",
    "    #preprossesing user's question\n",
    "    question = clean_text(question)\n",
    "    return [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in question.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  ['friend', 'of', 'yours', 'did', 'you', 'help', 'him', 'out', 'of', 'a', '<UNK>']\n",
      "Answer ['i', 'do', 'not', 'know', 'i', 'am', 'not', 'to', '<UNK>', '<EOS>']\n",
      "\n",
      "Question:  ['you', 'call', 'that', 'john', '<UNK>', '<UNK>', 'shit', 'dancing', 'i', 'would', 'not', 'dance', 'like', 'that', 'in', 'private', 'if', 'you', 'paid', 'me']\n",
      "Answer ['i', 'am', 'not', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['well', 'we', 'will', 'see', 'you', 'later', 'bob']\n",
      "Answer ['i', 'am', 'not', 'going', 'to', '<UNK>', '<EOS>', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['what', 'get', 'away', '<UNK>']\n",
      "Answer ['i', 'am', 'not', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['<UNK>', 'for', 'the', 'rest', 'of', 'the', 'year']\n",
      "Answer ['i', 'am', 'not', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['i', 'have', 'been', 'given', 'nothing', 'god', 'makes', 'men', 'what', 'they', 'are']\n",
      "Answer ['i', 'am', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['two', 'at', 'nine', 'perfect']\n",
      "Answer ['i', 'am', 'not', 'going', 'to', 'be', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['good', 'luck', 'on', 'your', 'mission', 'sir', 'by', '<UNK>', '<UNK>', 'by', 'the', '<UNK>', 'of', '<UNK>', 'i', 'wish', 'you']\n",
      "Answer ['i', 'am', 'not', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['last', 'month', 'the', '<UNK>', 'of', '<UNK>', 'spent', 'no', 'more']\n",
      "Answer ['i', 'am', 'not', '<UNK>', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four', 'four', 'four', 'four', 'four', 'four', 'four', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['he', 'was', 'a', 'nice', 'guy']\n",
      "Answer ['i', 'am', 'not', '<UNK>', 'to', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four', 'four', 'four', 'four', 'four', 'four', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['want', 'to', 'buy']\n",
      "Answer ['i', 'am', 'not', 'going', 'to', 'be', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['no', 'jason', 'please']\n",
      "Answer ['i', 'am', 'not', 'not', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['a', 'pleasure', 'pity', 'about', '<UNK>']\n",
      "Answer ['i', 'am', 'not', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['people', 'change', 'in', '<UNK>', 'to', 'each', 'other', 'love', '<UNK>', 'on', 'its', 'own']\n",
      "Answer ['i', 'am', 'not', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['what', 'about', 'your', 'friend', 'the', 'man', 'who', '<UNK>', 'this', 'building']\n",
      "Answer ['i', 'am', 'not', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four', 'four', 'four', 'four', 'four', 'four', 'four', 'four', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['guess', 'guess', 'majesty', 'imagine', 'the', '<UNK>', 'time', 'such', 'a', 'thing', 'could', 'last', 'then', 'double', 'it']\n",
      "Answer ['i', 'am', 'sorry', 'i', 'am', 'not', '<UNK>', '<EOS>', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['patrick', 'we', 'are', 'in', 'the', 'middle', 'of']\n",
      "Answer ['i', 'am', 'not', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  [\"let's\", 'get', 'you', '<UNK>']\n",
      "Answer ['i', 'am', 'not', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['at', 'least', 'you', 'and', 'kendall', 'agree', 'on', 'that']\n",
      "Answer ['i', 'am', 'not', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['did', 'you', 'kill', '<UNK>']\n",
      "Answer ['i', 'am', 'not', '<UNK>', '<UNK>', '<EOS>', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['no', 'just', 'pool']\n",
      "Answer ['i', 'have', 'not', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['she', 'is', 'from', '<UNK>']\n",
      "Answer ['i', 'do', 'not', 'know', '<EOS>', 'four', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['since', 'that', 'guy', '<UNK>', 'me', 'in', '<UNK>', '<UNK>']\n",
      "Answer ['i', 'am', 'not', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['sorry', 'mom']\n",
      "Answer ['i', 'am', 'not', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['lucky', 'nothing', 'i', 'had', 'to', 'empty', 'my', 'damn', 'gun', 'into', 'him']\n",
      "Answer ['i', 'am', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['you', 'and', 'the', 'other', 'one', 'you', 'are', 'still', '<UNK>', 'girls', 'you', 'always', 'were', 'his', 'girls']\n",
      "Answer ['i', 'am', 'not', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['see', 'i', 'knew', 'you', 'would', 'be', 'mad']\n",
      "Answer ['i', 'am', 'not', '<UNK>', '<UNK>', '<EOS>', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['where', 'is', 'he']\n",
      "Answer ['i', 'am', 'not', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['good', 'morning']\n",
      "Answer ['i', 'am', 'sorry', '<EOS>', 'four', 'four', 'four', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['what', '<UNK>']\n",
      "Answer ['i', 'am', 'not', 'going', 'to', 'you', 'are', '<UNK>', '<EOS>', 'four', 'four']\n",
      "\n",
      "Question:  ['smith', 'get', 'out', 'of', 'there']\n",
      "Answer ['i', 'do', 'not', 'know', 'i', 'am', '<UNK>', '<EOS>', 'four']\n",
      "\n",
      "Question:  ['it', 'turned', 'up', 'on', 'the', 'black', '<UNK>', 'one', 'of', 'my', '<UNK>', 'thought', 'i', 'might', 'be', 'interested']\n",
      "Answer ['i', 'am', 'not', 'going', 'to', 'be', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['what', 'happened', 'to', 'your', 'pants']\n",
      "Answer ['i', 'am', 'not', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['i', 'do', 'not', 'want', 'to']\n",
      "Answer ['i', 'am', 'not', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['old', '<UNK>', 'are', 'the', 'best', '<UNK>', 'eh']\n",
      "Answer ['i', 'am', 'sorry', '<EOS>', 'four', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['does', 'mom', 'know', 'you', 'have', 'company']\n",
      "Answer ['i', 'am', 'not', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['i', 'will', 'help', 'her', 'i', 'promise', 'but', 'i', 'think', 'you', 'should', 'tell', 'me', 'where', 'the', 'stones', 'are']\n",
      "Answer ['i', 'am', 'not', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['the', 'army', 'can', '<UNK>', 'without', 'me']\n",
      "Answer ['i', 'am', 'sorry', '<EOS>', 'four', 'four', 'four', 'four', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['i', 'think', 'so']\n",
      "Answer ['i', 'am', 'not', 'not', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['the', 'machine', 'works', 'and', '<UNK>', 'gone', 'mister', '<UNK>', 'on', 'us']\n",
      "Answer ['i', 'am', 'not', '<UNK>', '<UNK>', '<EOS>', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['nice', 'car', 'how', 'much', 'did', 'you', 'pay', 'for', 'it']\n",
      "Answer ['i', 'am', 'not', '<UNK>', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['bye', '<UNK>']\n",
      "Answer ['i', 'am', 'not', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['who', 'is', 'on', 'this', '<UNK>']\n",
      "Answer ['i', 'am', 'not', '<UNK>', '<UNK>', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['but', 'i', 'do', 'not', 'understand']\n",
      "Answer ['i', 'am', 'not', '<UNK>', '<UNK>', '<UNK>', '<EOS>', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['i', 'lost', 'my', 'head', 'i', 'am', 'sorry', 'i', 'do', 'not', 'know', 'what', 'happened']\n",
      "Answer ['i', 'am', 'not', '<UNK>', 'i', 'am', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['you', 'found', 'us', 'okay']\n",
      "Answer ['i', 'am', 'not', '<EOS>', 'four', 'four', 'four', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['good', 'luck']\n",
      "Answer ['i', 'do', 'not', 'know', '<EOS>', 'four', 'four', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['what', 'did', 'you', 'wish', 'for', 'adam']\n",
      "Answer ['i', 'am', 'not', 'going', 'to', 'be', '<UNK>', '<EOS>', 'four', 'four']\n",
      "\n",
      "Question:  ['i', 'lost', 'my', 'head', 'i', 'am', 'sorry', 'i', 'do', 'not', 'know', 'what', 'happened']\n",
      "Answer ['i', 'am', 'not', '<UNK>', '<UNK>', '<EOS>', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['this', '<UNK>', 'really', 'bad']\n",
      "Answer ['i', 'am', 'not', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['<UNK>', '<UNK>', 'worth', 'like', 'four', 'thousand', 'dollars']\n",
      "Answer ['i', 'am', 'sorry', 'i', 'am', 'sorry', '<EOS>', 'four', 'four']\n",
      "\n",
      "Question:  ['no', 'we', 'have', 'a', 'stone', 'killer', 'trying', 'to', 'make', 'a', 'point']\n",
      "Answer ['i', 'am', 'not', 'sorry', '<EOS>', 'four', 'four', 'four', 'four']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  ['try', 'again']\n",
      "Answer ['i', 'do', 'not', 'know', 'i', 'do', 'not', 'have', 'to', '<UNK>', '<EOS>']\n",
      "\n",
      "Question:  ['no', 'i', 'am', 'serious', 'that', '<UNK>', 'got', 'exactly', 'what', 'she', '<UNK>', 'yes']\n",
      "Answer ['i', 'am', 'not', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['what', 'do', 'i', 'get']\n",
      "Answer ['i', 'am', 'not', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['do', 'not', 'worry', 'it', 'is', 'really', '<UNK>', 'this', 'time', 'of', 'year']\n",
      "Answer ['i', 'am', 'not', 'afraid', '<EOS>', 'four', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['what', 'are', 'you', 'doing', 'here']\n",
      "Answer ['i', 'do', 'not', 'know', '<EOS>', 'four', 'four', 'four', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['just', 'place', 'an', '<UNK>', 'in', 'the', '<UNK>', 'place', 'on', 'the', '<UNK>']\n",
      "Answer ['i', 'do', 'not', 'know', 'i', 'am', '<UNK>', '<EOS>', 'four']\n",
      "\n",
      "Question:  ['i', 'would', 'not', 'know']\n",
      "Answer ['i', 'am', 'not', '<UNK>', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['fuck', '<UNK>', 'you', 'are', 'gonna', 'end', 'up', '<UNK>', '<UNK>', '<UNK>', 'to', 'a', '<UNK>', '<UNK>']\n",
      "Answer ['i', 'am', 'not', 'not', 'a', '<UNK>', '<EOS>', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['all', 'right', 'austin', 'i', 'think', 'you', 'should', 'go']\n",
      "Answer ['i', 'am', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['you', 'look', 'like', 'an', '<UNK>', 'rock', '<UNK>']\n",
      "Answer ['i', 'am', 'not', '<UNK>', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['i', 'can', 'knock', 'a', 'man', 'out', 'with', 'a', 'six', '<UNK>', '<UNK>']\n",
      "Answer ['i', 'am', 'sorry', '<EOS>', 'four', 'four', 'four', 'four', 'four', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['watching', 'television', 'in', 'color']\n",
      "Answer ['i', 'am', 'not', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['maybe', 'i', 'better', 'ask', 'around', 'see', 'what', 'your', '<UNK>', 'think']\n",
      "Answer ['i', 'am', 'not', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['did', 'you', 'shoot', 'the', '<UNK>']\n",
      "Answer ['i', 'am', 'not', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['am', 'i', '<UNK>', 'to', 'be', '<UNK>']\n",
      "Answer ['i', 'am', 'not', '<EOS>', 'four', 'four', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['chief', 'says']\n",
      "Answer ['i', 'am', 'not', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four', 'four', 'four', 'four', 'four', 'four', 'four', 'four', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['you', 'got', 'a', '<UNK>', 'from', '<UNK>', 'today']\n",
      "Answer ['i', 'am', 'not', 'not', '<UNK>', '<EOS>', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['this', 'this', 'this', 'is', 'bullshit']\n",
      "Answer ['i', 'am', 'not', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['hello', 'it', 'is', '<UNK>', '<UNK>']\n",
      "Answer ['i', 'do', 'not', '<EOS>', 'four', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['it', 'is', 'magic']\n",
      "Answer ['i', 'do', 'not', 'know', 'i', 'am', '<UNK>', '<EOS>', 'four']\n",
      "\n",
      "Question:  ['i', 'am', 'feeling', 'fine', 'this', 'morning']\n",
      "Answer ['i', 'am', 'not', 'a', '<UNK>', '<EOS>', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['i', 'am', 'not', 'much', 'of', 'a', '<UNK>', 'really', 'i', 'just', 'like', 'this', 'bar']\n",
      "Answer ['i', 'am', 'not', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['is', 'it', 'the', 'medicine', 'that', 'is', 'making', 'you', '<UNK>']\n",
      "Answer ['i', 'am', 'not', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['they', 'went', 'crazy']\n",
      "Answer ['i', 'am', 'not', 'sorry', '<EOS>', 'four', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['and', 'it', 'is', 'called']\n",
      "Answer ['i', 'am', 'not', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['you', 'are', 'a', '<UNK>']\n",
      "Answer ['i', 'am', 'sorry', '<EOS>', 'four', 'four', 'four', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['yeah', 'but', 'sometimes', 'i', 'wish', 'i', 'could', 'meet', 'mine', 'anyway']\n",
      "Answer ['i', 'am', 'not', '<UNK>', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['how', 'about', 'him', 'not', '<UNK>', 'the', '<UNK>', '<UNK>', 'how', 'about', 'that']\n",
      "Answer ['i', 'do', 'not', 'know', 'i', 'am', 'not', '<UNK>', '<EOS>']\n",
      "\n",
      "Question:  ['oh', 'mr', 'dickson', 'they', 'are', 'going', 'to', 'arrest', 'matt', 'they', 'think', 'he', 'did', 'it']\n",
      "Answer ['i', 'do', 'not', 'know', 'i', 'am', '<UNK>', '<EOS>', 'four']\n",
      "\n",
      "Question:  ['what', 'did', 'you', 'see', 'what', 'did', 'it', 'look', 'like']\n",
      "Answer ['i', 'am', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['that', 'is', 'it', 'and', 'you', 'keep', '<UNK>', 'to', 'go', 'to', 'the', '<UNK>']\n",
      "Answer ['i', 'am', 'not', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['it', 'is', 'the', 'name', 'of', 'my', 'father', 'and', 'his', 'father', 'before', 'me']\n",
      "Answer ['i', 'am', 'sorry', 'i', 'am', 'not', '<UNK>', '<EOS>', 'four']\n",
      "\n",
      "Question:  ['it', 'was', '<UNK>']\n",
      "Answer ['i', 'am', 'not', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['okay', 'i', 'guess']\n",
      "Answer ['i', 'am', 'sorry', '<EOS>', 'four', 'four', 'four', 'four', 'four', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['how', 'long', 'will', 'we', 'have', 'to', 'stay', 'down', 'here']\n",
      "Answer ['i', 'am', 'not', '<UNK>', 'to', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['mrs', '<UNK>', 'your', 'daughter', 'is', 'dead', 'she', 'is', 'dead']\n",
      "Answer ['i', 'am', 'sorry', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['yes', 'i', 'want', 'this', 'over', 'and', 'done', 'with']\n",
      "Answer ['i', 'am', 'not', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['he', 'stole', 'a', 'pair', 'of', 'your', 'panties', 'while', 'you', 'were', 'being', '<UNK>']\n",
      "Answer ['i', 'am', 'not', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['where', 'was', 'it', '<UNK>']\n",
      "Answer ['i', 'am', 'not', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['that', '<UNK>', 'son', 'of', 'a', 'bitch']\n",
      "Answer ['i', 'do', 'not', 'know', 'i', 'am', 'not', '<UNK>', '<EOS>', 'four', 'four']\n",
      "\n",
      "Question:  ['neither', 'does', 'he', 'he', 'looks', '<UNK>']\n",
      "Answer ['i', 'am', 'sorry', '<EOS>', 'four', 'four', 'four', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['do', 'not', 'worry', 'about', 'it', 'we', 'will', 'find', 'out', 'when', 'it', 'goes', 'bad']\n",
      "Answer ['i', 'am', 'not', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['not', 'too', 'bad']\n",
      "Answer ['i', 'do', 'not', 'know', '<EOS>', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['i', 'am', 'not', 'crazy', 'i', 'know', 'the', 'difference', 'between', 'right', 'and', 'wrong']\n",
      "Answer ['i', 'am', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['thanks', 'but', 'we', 'have', 'a', '<UNK>', 'woman', 'in', 'on', '<UNK>']\n",
      "Answer ['i', 'do', 'not', 'have', 'been', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four', 'four', 'four', 'four', 'four', 'four', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['shut', 'up']\n",
      "Answer ['i', 'do', 'not', 'know', '<EOS>', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['sebastian', 'does', 'not', 'like', 'to', 'go', 'out', 'too', 'much']\n",
      "Answer ['i', 'am', '<UNK>', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['we', 'are', 'with', 'the', 'police', 'department', 'this', 'is', 'sergeant', '<UNK>', 'and', 'i', 'am']\n",
      "Answer ['i', 'am', 'not', 'going', 'to', '<EOS>', 'four', 'four', 'four', 'four']\n",
      "\n",
      "Question:  ['do', 'you', 'have', 'mrs', 'george', 'for', 'english']\n",
      "Answer ['i', 'am', 'not', '<UNK>', '<EOS>', 'four', 'four', 'four', 'four', 'four', 'four']\n"
     ]
    }
   ],
   "source": [
    "counter=-1\n",
    "#testing for 100 questions\n",
    "while counter<100:\n",
    "    counter+=1\n",
    "    graph = tf.Graph().as_default() #creating default graph\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:  # your session object\n",
    "\n",
    "        saver.restore(sess, \"./my-model\")\n",
    "        # Restoring our best model\n",
    "\n",
    "\n",
    "        # Random input question\n",
    "        random = np.random.choice(len(short_questions))\n",
    "        input_question = short_questions[random]\n",
    "\n",
    "        # Prepare the question\n",
    "        input_question = question_to_seq(input_question, questions_vocab_to_int)\n",
    "\n",
    "        # Padding\n",
    "        input_question = input_question + [questions_vocab_to_int[\"<PAD>\"]] * (max_line_length - len(input_question))\n",
    "        # Add empty questions so as to fill the batch\n",
    "        batch_shell = np.zeros((batch_size, max_line_length))\n",
    "        # User's input question\n",
    "        batch_shell[0] = input_question    \n",
    "\n",
    "        # Run the model with the input question\n",
    "        answer_logits = sess.run(inference_logits, {input_data: batch_shell, \n",
    "                                                    keep_prob: 0.3})[0]\n",
    "\n",
    "        # Remove the padding from the Question and Answer\n",
    "        pad_q = questions_vocab_to_int[\"<PAD>\"]\n",
    "        pad_a = answers_vocab_to_int[\"<PAD>\"]\n",
    "        \n",
    "       \n",
    "        print('\\nQuestion: '), str([questions_int_to_vocab[i] for i in input_question if i != pad_q])\n",
    "        print('Answer'), str([answers_int_to_vocab[i] for i in np.argmax(answer_logits, 1) if i != pad_a])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
